name: Performance Benchmarks

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:  # Allow manual trigger

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  benchmarks:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Cache cargo registry
      uses: actions/cache@v4
      with:
        path: ~/.cargo/registry
        key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}

    - name: Cache cargo index
      uses: actions/cache@v4
      with:
        path: ~/.cargo/git
        key: ${{ runner.os }}-cargo-git-${{ hashFiles('**/Cargo.lock') }}

    - name: Cache cargo build
      uses: actions/cache@v4
      with:
        path: target
        key: ${{ runner.os }}-cargo-bench-target-${{ hashFiles('**/Cargo.lock') }}

    # Run backend selection benchmarks
    - name: Run backend selection benchmarks
      run: |
        echo "ğŸ“Š Running backend selection benchmarks..."
        cargo bench --bench backend_selection -- --output-format bencher | tee backend_selection.txt

    # Run ML converter benchmarks
    - name: Run converter performance benchmarks
      run: |
        echo "ğŸ“Š Running ML converter benchmarks..."
        cargo bench --bench converter_performance -- --output-format bencher | tee converter_performance.txt

    # Generate summary report
    - name: Generate benchmark summary
      run: |
        echo "# Benchmark Results Summary" > benchmark_summary.md
        echo "" >> benchmark_summary.md
        echo "## Backend Selection Performance" >> benchmark_summary.md
        echo "" >> benchmark_summary.md
        echo "Validates MoE (Mixture-of-Experts) algorithm and 5Ã— PCIe rule." >> benchmark_summary.md
        echo "" >> benchmark_summary.md
        echo "\`\`\`" >> benchmark_summary.md
        grep -E "(time:|elements)" backend_selection.txt | head -20 >> benchmark_summary.md || true
        echo "\`\`\`" >> benchmark_summary.md
        echo "" >> benchmark_summary.md
        echo "## ML Converter Performance" >> benchmark_summary.md
        echo "" >> benchmark_summary.md
        echo "Measures NumPy, sklearn, and PyTorch conversion overhead." >> benchmark_summary.md
        echo "" >> benchmark_summary.md
        echo "\`\`\`" >> benchmark_summary.md
        grep -E "(time:|elements)" converter_performance.txt | head -20 >> benchmark_summary.md || true
        echo "\`\`\`" >> benchmark_summary.md
        echo "" >> benchmark_summary.md
        echo "---" >> benchmark_summary.md
        echo "ğŸ“ˆ Full HTML reports available in artifacts" >> benchmark_summary.md
        cat benchmark_summary.md

    # Upload criterion HTML reports
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-reports
        path: |
          target/criterion/
          backend_selection.txt
          converter_performance.txt
          benchmark_summary.md
        retention-days: 30

    # Upload summary as artifact
    - name: Upload benchmark summary
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-summary
        path: benchmark_summary.md
        retention-days: 90

    - name: Benchmark completion
      run: |
        echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
        echo "â•‘  âœ… Performance Benchmarks Completed          â•‘"
        echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        echo "ğŸ“Š Benchmarks executed:"
        echo "  âœ… Backend selection (CPU/SIMD/GPU MoE)"
        echo "  âœ… ML converter performance"
        echo "  âœ… Conversion overhead validation"
        echo ""
        echo "ğŸ“ˆ Results available in artifacts"
        echo "ğŸ¯ Check criterion HTML reports for detailed analysis"

  # Optional: Run with trueno-integration feature if available
  benchmarks-with-trueno:
    name: Benchmarks with Trueno Integration
    runs-on: ubuntu-latest
    continue-on-error: true  # Don't fail if trueno not available

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable

    - name: Cache cargo registry
      uses: actions/cache@v4
      with:
        path: ~/.cargo/registry
        key: ${{ runner.os }}-cargo-registry-trueno-${{ hashFiles('**/Cargo.lock') }}

    - name: Cache cargo build
      uses: actions/cache@v4
      with:
        path: target
        key: ${{ runner.os }}-cargo-bench-trueno-${{ hashFiles('**/Cargo.lock') }}

    - name: Run benchmarks with trueno
      run: |
        echo "ğŸ“Š Running benchmarks with trueno integration..."
        cargo bench --bench backend_selection --features trueno-integration -- --output-format bencher | tee trueno_benchmarks.txt
      continue-on-error: true

    - name: Upload trueno benchmark results
      uses: actions/upload-artifact@v4
      if: success()
      with:
        name: trueno-benchmark-reports
        path: |
          target/criterion/
          trueno_benchmarks.txt
        retention-days: 30

  benchmark-success:
    name: Benchmark Success
    runs-on: ubuntu-latest
    needs: [benchmarks]

    steps:
    - name: All benchmarks completed
      run: |
        echo "âœ… All performance benchmarks completed successfully"
        echo "ğŸš€ Performance validation passed"
